import hashlib
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Optional

from dataclasses_json import DataClassJsonMixin
from tokenizers import Tokenizer
from torch import nn

from linear_relational.lib.torch_utils import guess_model_name
from linear_relational.lib.verify_answers_match_expected import (
    verify_answers_match_expected,
)
from linear_relational.Prompt import Prompt


@dataclass
class MatchingPromptsCache(DataClassJsonMixin):
    cache: dict[str, bool]
    model_name: str


class PromptValidator:
    """
    PromptValidator filters prompts based on whether the model's answers match the expected answers.
    This class can cache results to avoid repeating work between runs.
    """

    _cache: MatchingPromptsCache
    cache_file: str | Path | None
    model: nn.Module
    tokenizer: Tokenizer

    def __init__(
        self,
        model: nn.Module,
        tokenizer: Tokenizer,
        cache_file: Optional[str | Path] = None,
        load_saved_cache: bool = True,
    ) -> None:
        self.model = model
        self.tokenizer = tokenizer
        model_name = guess_model_name(model)
        self.cache_file = cache_file
        if cache_file and load_saved_cache and os.path.exists(cache_file):
            with open(cache_file, "r") as f:
                self._cache = MatchingPromptsCache.from_json(f.read())
                if self._cache.model_name != model_name:
                    raise ValueError(
                        f"Cache file {cache_file} was generated by a different model"
                    )
        else:
            self._cache = MatchingPromptsCache(cache={}, model_name=model_name)

    def write_cache(self, cache_file: Optional[str | Path] = None) -> None:
        _cache_file = cache_file or self.cache_file
        if _cache_file is None:
            raise ValueError("No cache file was provided")
        with open(_cache_file, "w") as f:
            f.write(self._cache.to_json())

    def _is_cached(self, prompt: Prompt) -> bool:
        key = cache_key(prompt.text, prompt.answer)
        return key in self._cache.cache

    def _prompt_matches(self, prompt: Prompt) -> bool:
        key = cache_key(prompt.text, prompt.answer)
        return self._cache.cache[key]

    def filter_prompts(
        self,
        prompts: Iterable[Prompt],
        batch_size: int = 8,
        show_progress: bool = False,
    ) -> list[Prompt]:
        uncached_prompts = [prompt for prompt in prompts if not self._is_cached(prompt)]
        if len(uncached_prompts) > 0:
            answer_match_results = verify_answers_match_expected(
                model=self.model,
                tokenizer=self.tokenizer,
                prompts=[prompt.text for prompt in uncached_prompts],
                expected_answers=[prompt.answer for prompt in uncached_prompts],
                batch_size=batch_size,
                show_progress=show_progress,
            )
            for prompt, match_result in zip(uncached_prompts, answer_match_results):
                key = cache_key(prompt.text, prompt.answer)
                self._cache.cache[key] = match_result.answer_matches_expected
        return [prompt for prompt in prompts if self._prompt_matches(prompt)]


def cache_key(prompt_text: str, answer: str) -> str:
    # return a md5 hash of the prompt text and answer
    return hashlib.md5((prompt_text + answer).encode("utf-8")).hexdigest()[:15]
